---
title: "Response Events"
description: "AI response generation and streaming events"
---

Response events handle AI response generation and streaming. These events provide real-time updates as the AI generates its text and audio responses.

## Event Types

### `response.audio_transcript.delta`
Triggered for each incremental text chunk as the AI generates its response.

**Event data**:
- `delta`: The new text chunk
- `response_id`: Unique identifier for this response

**When to handle**: Stream the text to your UI as it arrives, supporting markdown rendering.

<CodeGroup>
```javascript JavaScript
case "response.audio_transcript.delta":
  playVideo = true;
  const transcript = data.delta;
  const responseId = data.response_id;
  
  // Accumulate markdown content
  if (!markdownBuffer.has(responseId)) {
    markdownBuffer.set(responseId, "");
  }
  
  const existingBuffer = markdownBuffer.get(responseId);
  markdownBuffer.set(responseId, existingBuffer + transcript);
  
  // Get or create message container
  let aiMessageSpan = responseSpans.get(responseId);
  
  if (!aiMessageSpan) {
    const aiMessageContainer = document.createElement('div');
    aiMessageContainer.classList.add('character-chat-item', 'item-character');
    
    aiMessageSpan = document.createElement('span');
    aiMessageSpan.classList.add('markdown-content');
    aiMessageContainer.appendChild(aiMessageSpan);
    
    const chatContainer = document.querySelector('.ah-character-chat');
    chatContainer.appendChild(aiMessageContainer);
    
    responseSpans.set(responseId, aiMessageSpan);
  }
  
  // Render markdown
  const fullContent = markdownBuffer.get(responseId);
  const parsedContent = marked.parse(fullContent);
  aiMessageSpan.innerHTML = parsedContent;
  Prism.highlightAllUnder(aiMessageSpan);
  
  // Auto-scroll
  const chatContainer = document.querySelector('.ah-character-chat');
  chatContainer.scrollTop = chatContainer.scrollHeight;
  break;
```

```python Python
if data.get('type') == 'response.audio_transcript.delta':
    play_video = True
    transcript = data.get('delta')
    response_id = data.get('response_id')
    
    # Accumulate markdown content
    if response_id not in markdown_buffer:
        markdown_buffer[response_id] = ""
    
    markdown_buffer[response_id] += transcript
    
    # Get or create message container
    if response_id not in response_spans:
        ai_message_container = create_ai_message_container()
        response_spans[response_id] = ai_message_container
    
    # Render markdown
    full_content = markdown_buffer[response_id]
    parsed_content = markdown.markdown(full_content)
    update_ai_message(response_id, parsed_content)
    
    # Auto-scroll
    scroll_chat_to_bottom()
```
</CodeGroup>

<Tip>
The `response_id` allows you to handle multiple concurrent responses. Always accumulate deltas by `response_id` to reconstruct the complete message.
</Tip>

### `response.audio_transcript.done`
Triggered when the AI has finished generating the complete text response.

**Event data**:
- `transcript`: The complete transcribed text of the AI response
- `response_id`: Unique identifier for this response

**When to handle**: Save the complete response to conversation history.

<CodeGroup>
```javascript JavaScript
case "response.audio_transcript.done":
  console.log("Received transcription: " + data.transcript);
  await appendRealtimeChatHistory("assistant", data.transcript);
  break;
```

```python Python
if data.get('type') == 'response.audio_transcript.done':
    transcript = data.get('transcript')
    print(f"Received transcription: {transcript}")
    await append_chat_history("assistant", transcript)
```
</CodeGroup>

### `response.audio.delta`
Triggered for each audio chunk as the AI generates speech audio.

**Event data**:
- `delta`: Base64-encoded audio chunk (optional, WebRTC is typically used instead)

**Note**: For WebRTC connections, audio is received via the WebRTC stream rather than WebSocket deltas.

<CodeGroup>
```javascript JavaScript
case "response.audio.delta":
  if (data.delta) {
    // Handle audio delta if using WebSocket for audio
    // Note: WebRTC is the recommended method for audio/video streaming
  }
  break;
```

```python Python
if data.get('type') == 'response.audio.delta':
    audio_delta = data.get('delta')
    if audio_delta:
        # Handle audio delta if using WebSocket for audio
        # Note: WebRTC is the recommended method for audio/video streaming
        pass
```
</CodeGroup>

### `response.audio.done`
Triggered when the AI has finished generating all audio for the response.

**When to handle**: Reset audio playback flags and prepare for next interaction.

<CodeGroup>
```javascript JavaScript
case "response.audio.done":
  console.log("Audio response complete.");
  isPlaying = false;
  playVideo = false;
  break;
```

```python Python
if data.get('type') == 'response.audio.done':
    print("Audio response complete.")
    is_playing = False
    play_video = False
```
</CodeGroup>

## Event Flow

```
AI starts generating
  ↓
Receive response.audio_transcript.delta (multiple times)
  ↓
Accumulate text by response_id, render markdown
  ↓
Start video playback
  ↓
Receive response.audio_transcript.done
  ↓
Save complete response to history
  ↓
Receive response.audio.done
  ↓
Reset playback flags
```

<Info>
**Streaming Example:**
- `delta: "Hello"` → Display "Hello"
- `delta: " there"` → Display "Hello there"
- `delta: "!"` → Display "Hello there!"
- `done: "Hello there!"` → Save complete message
</Info>

